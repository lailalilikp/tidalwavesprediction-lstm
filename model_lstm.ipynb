{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX91on_l872W"
      },
      "outputs": [],
      "source": [
        "pip install -U kaleido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvbrZSUF89Th"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkcIGjOGliNZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_final = pd.read_excel('drive/MyDrive/Tugas Akhir/Dataset/Per_30_Mins_Data Tidal Wave Surabaya for Training.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv7QWKzXliNb"
      },
      "outputs": [],
      "source": [
        "new_header = df_final.iloc[0] #grab the first row for the header\n",
        "df_final = df_final[1:] #take the data less the header row\n",
        "df_final.columns = new_header #set the header row as the df header"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNMGVEG4liNc"
      },
      "outputs": [],
      "source": [
        "df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os8VA253liNd"
      },
      "outputs": [],
      "source": [
        "# Set Index with Time\n",
        "df_final = df_final.set_index(\"Time (UTC)\")\n",
        "#main_data = df_final[['pr2(m)', 'prs(m)', 'rad(m)']].values.tolist()\n",
        "main_data = df_final[['pr2(m)', 'prs(m)']].values.tolist()\n",
        "main_data_index = df_final.index.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM6-a7EIliNd"
      },
      "outputs": [],
      "source": [
        "monthly = '5'\n",
        "\n",
        "# Obtain the test set first\n",
        "test_set = df_final.last('{}M'.format(monthly))\n",
        "#test_data = test_set[['pr2(m)', 'prs(m)', 'rad(m)']].values.tolist()\n",
        "test_data = test_set[['pr2(m)', 'prs(m)']].values.tolist()\n",
        "test_data_index = test_set.index.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpd7yrqpliNe"
      },
      "outputs": [],
      "source": [
        "# Get every data previously from the final test set\n",
        "max_date = test_set.index[0]\n",
        "train_set = df_final[:max_date]\n",
        "\n",
        "#train_data = train_set[['pr2(m)', 'prs(m)', 'rad(m)']].values.tolist()\n",
        "train_data = train_set[['pr2(m)', 'prs(m)']].values.tolist()\n",
        "train_data_index = train_set.index.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFObqnsdliNg"
      },
      "source": [
        "<h2>Normalization and Windowing Technique</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxRZ4uqaliNh"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize with standard scaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(main_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TPdpj9nliNh"
      },
      "outputs": [],
      "source": [
        "# Apply Normalization\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NofhuXmzliNi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps, date):\n",
        "    X, y, date_list = list(), list(), list()\n",
        "    for i in range(len(sequence)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "        # check if we are beyond the sequence\n",
        "        if end_ix > len(sequence)-1:\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        if date is not None:\n",
        "            seq_x, seq_y, seq_date = sequence[i:end_ix], sequence[end_ix], date[end_ix]\n",
        "            X.append(seq_x)\n",
        "            y.append(seq_y)\n",
        "            date_list.append(seq_date)\n",
        "        else:\n",
        "            seq_x, seq_y= sequence[i:end_ix], sequence[end_ix]\n",
        "            X.append(seq_x)\n",
        "            y.append(seq_y)\n",
        "            #date_list.append(seq_date)\n",
        "\n",
        "    return np.array(X), np.array(y), np.array(date_list)\n",
        "\n",
        "n_steps=16 #nyoba2, semakin panjang ambil x step nanti modelnya akan terlalu berpatokan pada data yang terlalu ke belakang. kalo semakin dikit data kekurangan konteks\n",
        "x_sequence_train, y_sequence_train, train_seq_date = split_sequence(train_data, n_steps=n_steps, date=train_data_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8Cc8iLMliNi"
      },
      "outputs": [],
      "source": [
        "x_sequence_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LChTBWMO5zim"
      },
      "outputs": [],
      "source": [
        "# Save normalized train and test data to Excel\n",
        "import pandas as pd\n",
        "\n",
        "# Convert train and test data back to DataFrame for saving\n",
        "train_data_normalized_df = pd.DataFrame(train_data, columns=['pr2(m)_normalized', 'prs(m)_normalized'], index=train_data_index)\n",
        "test_data_normalized_df = pd.DataFrame(test_data, columns=['pr2(m)_normalized', 'prs(m)_normalized'], index=test_data_index)\n",
        "\n",
        "# Save the train and test normalized data as Excel files\n",
        "train_data_normalized_df.to_excel('normalized_train_data.xlsx', sheet_name='Train Data')\n",
        "test_data_normalized_df.to_excel('normalized_test_data.xlsx', sheet_name='Test Data')\n",
        "\n",
        "# For Google Colab: Add functionality to download the Excel files\n",
        "from google.colab import files\n",
        "files.download('normalized_train_data.xlsx')\n",
        "files.download('normalized_test_data.xlsx')\n",
        "\n",
        "print(\"Train and test normalized data have been saved to Excel and are ready for download.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ19PyE9liNi"
      },
      "source": [
        "<h2>Define and Train the Model</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxBFQFeRliNj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "def create_model():\n",
        "    # Input Layers\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(n_steps, 2)),\n",
        "        Bidirectional(tf.keras.layers.LSTM(128, recurrent_regularizer=tf.keras.regularizers.L1(1e-5), return_sequences=True)),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        Bidirectional(tf.keras.layers.LSTM(256, recurrent_regularizer=tf.keras.regularizers.L1(1e-5), return_sequences=False)),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(128, activation=tf.keras.activations.swish),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Dense(64, activation=tf.keras.activations.swish),\n",
        "        tf.keras.layers.Dense(2)\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAjF_Li3liNj"
      },
      "outputs": [],
      "source": [
        "model = create_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1CEn73kliNj"
      },
      "outputs": [],
      "source": [
        "def scheduler(epoch:int) -> float:\n",
        "    if epoch < 4:\n",
        "        return 0.01\n",
        "    elif epoch < 30:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 0.0001\n",
        "\n",
        "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=scheduler(0)),\n",
        "    loss=tf.keras.losses.Huber(),\n",
        "    metrics=['mae', 'mse']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN5qKNYxliNk"
      },
      "outputs": [],
      "source": [
        "model_history = model.fit(x_sequence_train, y_sequence_train, epochs=5, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ1yM38iliNk"
      },
      "source": [
        "<h2>Evaluate</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_UojPZoliNl"
      },
      "outputs": [],
      "source": [
        "final_loss = model_history.history['loss'][-1]\n",
        "final_mae = model_history.history['mae'][-1]\n",
        "final_mse = model_history.history['mse'][-1]\n",
        "\n",
        "print(\"Model Loss : {}\".format(final_loss))\n",
        "print(\"Final MAE : {}\".format(final_mae))\n",
        "print(\"Final MSE : {}\".format(final_mse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLJownmUliNl"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "# Append the final 4\n",
        "evaluation_list = [x for x in train_data[-n_steps:]]\n",
        "evaluation_list_data_index = [x for x in train_data_index[-n_steps:]]\n",
        "\n",
        "for idx, x in enumerate(test_data):\n",
        "    evaluation_list.append(x)\n",
        "    evaluation_list_data_index.append(test_data_index[idx])\n",
        "\n",
        "x_sequence_eval, y_sequence_eval, date_seq_eval = split_sequence(evaluation_list, n_steps=n_steps, date=evaluation_list_data_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCRLBMeIliNl"
      },
      "outputs": [],
      "source": [
        "final_loss_pred, final_mae_pred, final_mse_pred = model.evaluate(x_sequence_eval, y_sequence_eval)\n",
        "predicted_result = model.predict(x_sequence_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8BnEBDyliNm"
      },
      "outputs": [],
      "source": [
        "print(\"Model Loss : {}\".format(final_loss_pred))\n",
        "print(\"Final MAE : {}\".format(final_mae_pred))\n",
        "print(\"Final MSE : {}\".format(final_mse_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLVjBpQxliNm"
      },
      "outputs": [],
      "source": [
        "# Reverse Transform\n",
        "# Retransform the predicted result\n",
        "predict_result = scaler.inverse_transform(predicted_result)\n",
        "print(len(predicted_result))\n",
        "\n",
        "# Get the predicted result\n",
        "first_sensors, second_sensors, third_sensors = [], [], []\n",
        "for result in predict_result:\n",
        "    first_sensors.append(result[0])\n",
        "    second_sensors.append(result[1])\n",
        "    #third_sensors.append(result[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RFdP-AlliNm"
      },
      "outputs": [],
      "source": [
        "# Separate by index\n",
        "#index_1 = df_final[['pr2(m)', 'prs(m)', 'rad(m)']].values.tolist()\n",
        "index_1 = df_final[['pr2(m)', 'prs(m)']].values.tolist()\n",
        "value_2 = np.array(df_final[['pr2(m)']].values.tolist()).reshape(-1, )\n",
        "value_3 = np.array(df_final[['prs(m)']].values.tolist()).reshape(-1, )\n",
        "#value_4 = np.array(df_final[['rad(m)']].values.tolist()).reshape(-1, )\n",
        "y = [integer for integer in range(len(value_3))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOB9jizlliNm"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POlAaGcbliNn"
      },
      "outputs": [],
      "source": [
        "def make_plot(column_name, datetime_main, datetime_pred, value_main, value_pred, save_path:str):\n",
        "    fig = make_subplots(rows=1, cols=1)\n",
        "\n",
        "    trace1 = go.Scatter(x=datetime_main, y=value_main, mode='markers', name='Data Real {}'.format(column_name))\n",
        "    trace2 = go.Scatter(x=datetime_pred, y=value_pred, mode='lines', name='Data Prediksi {}'.format(column_name))\n",
        "\n",
        "    fig.add_trace(trace1)\n",
        "    fig.add_trace(trace2)\n",
        "\n",
        "    fig.update_layout(title='Prediction of {}'.format(column_name),\n",
        "                    xaxis_title='Tanggal',\n",
        "                    yaxis_title='Ketinggian air laut')\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    if save_path != '':\n",
        "        fig.write_image(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akshExGjliNn"
      },
      "source": [
        "<h2>Plot and document result</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29HUrBAyliNn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Saving Name Folder\n",
        "saving_dir_name = 'drive/MyDrive/Tugas Akhir/Result'\n",
        "\n",
        "# Initialize Directory\n",
        "if os.path.isdir(saving_dir_name) is False:\n",
        "    os.mkdir(saving_dir_name)\n",
        "\n",
        "# Document the data\n",
        "final_txt_eval = \"train_loss = {}\\ntrain_mse = {}\\ntrain_mae = {}\\ntest_loss = {}\\ntest_mse = {}\\ntest_mae = {}\".format(\n",
        "    final_loss,\n",
        "    final_mse,\n",
        "    final_mae,\n",
        "    final_loss_pred,\n",
        "    final_mse_pred,\n",
        "    final_mae_pred\n",
        ")\n",
        "\n",
        "# Save evaluation result\n",
        "final_saving_txt_path = os.path.join(saving_dir_name, \"plain_evaluation.txt\")\n",
        "with open(final_saving_txt_path, 'w') as f:\n",
        "    f.write(final_txt_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wYwyFPiliNn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig_model_loss = plt.gcf()\n",
        "# plt.plot(model_history.history['mae'][10:])\n",
        "# plt.plot(model_history.history['mse'][10:])\n",
        "# plt.plot(model_history.history['loss'][10:])\n",
        "plt.plot(model_history.history['mae'])\n",
        "plt.plot(model_history.history['mse'])\n",
        "plt.plot(model_history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['mae', 'mse', 'loss'], loc='upper left')\n",
        "plt.savefig(os.path.join(saving_dir_name, 'loss.jpg'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUQm6lcRliNo"
      },
      "outputs": [],
      "source": [
        "# First Sensor\n",
        "make_plot(\n",
        "    column_name = 'pr2(m)',\n",
        "    datetime_main = main_data_index,\n",
        "    datetime_pred = test_data_index,\n",
        "    value_main=value_2,\n",
        "    value_pred=first_sensors,\n",
        "    save_path=os.path.join(saving_dir_name, 'pr2.jpg')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AiIRidWliNo"
      },
      "outputs": [],
      "source": [
        "# Second Sensor\n",
        "make_plot(\n",
        "    column_name = 'prs(m)',\n",
        "    datetime_main = main_data_index,\n",
        "    datetime_pred = test_data_index,\n",
        "    value_main=value_3,\n",
        "    value_pred=second_sensors,\n",
        "    save_path=os.path.join(saving_dir_name, 'prs.jpg')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVzX3saFliNp"
      },
      "outputs": [],
      "source": [
        "# # Third Sensor\n",
        "# make_plot(\n",
        "#     column_name = 'rad(m)',\n",
        "#     datetime_main = main_data_index,\n",
        "#     datetime_pred = test_data_index,\n",
        "#     value_main=value_4,\n",
        "#     value_pred=third_sensors,\n",
        "#     save_path=os.path.join(saving_dir_name, 'rad.jpg')\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UlUthIeliNp"
      },
      "source": [
        "<h2>Second Round Evaluation</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhn9XhaaliNp"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def separate_eval(month: int, year: int, save_name: str, evaluate_5_months: bool = False):\n",
        "    if evaluate_5_months:\n",
        "        #Time frame for evaluation: 5 months.\n",
        "        start_date = pd.Timestamp(year=year, month=month, day=1) - pd.DateOffset(months=4)\n",
        "        end_date = pd.Timestamp(year=year, month=month, day=1) + pd.DateOffset(months=1)\n",
        "\n",
        "        #Filter data for a 5-month time range.\n",
        "        selected_row_data = df_final.loc[\n",
        "            (df_final.index >= start_date) & (df_final.index < end_date)\n",
        "        ][['pr2(m)', 'prs(m)']]\n",
        "    else:\n",
        "        #Specific month evaluation\n",
        "        selected_row_data = df_final.loc[\n",
        "            (df_final.index.month == month) & (df_final.index.year == year)\n",
        "        ][['pr2(m)', 'prs(m)']]\n",
        "\n",
        "    #Add 15 previous data points to provide context.\n",
        "    minimal_row_data = selected_row_data.index[0]\n",
        "    append_data = df_final.loc[:minimal_row_data][-15:][['pr2(m)', 'prs(m)']]\n",
        "\n",
        "    #Combine the main data and the additional data\n",
        "    merged_dataframe = pd.concat([selected_row_data, append_data]).sort_index()\n",
        "\n",
        "    #data conversion for evaluation\n",
        "    evaluation_list_data_index = selected_row_data.index.tolist()\n",
        "    selected_row_data_final = merged_dataframe.values.tolist()\n",
        "    selected_row_data_final = scaler.transform(selected_row_data_final)\n",
        "\n",
        "    #Convert the data into a sequence for prediction.\n",
        "    x_sequence_eval, y_sequence_eval, date_seq_eval = split_sequence(\n",
        "        selected_row_data_final, n_steps=n_steps, date=None\n",
        "    )\n",
        "\n",
        "    #Prediction and Evaluation\n",
        "    predicted_value = model.predict(x_sequence_eval)\n",
        "    predicted_value = scaler.inverse_transform(predicted_value)\n",
        "    final_loss_pred, final_mae_pred, final_mse_pred = model.evaluate(x_sequence_eval, y_sequence_eval)\n",
        "\n",
        "    print(\"Model Loss : {}\".format(final_loss_pred))\n",
        "    print(\"Final MAE : {}\".format(final_mae_pred))\n",
        "    print(\"Final MSE : {}\".format(final_mse_pred))\n",
        "\n",
        "    #Save the evaluation results to a file\n",
        "    final_txt_eval = \"test_loss = {}\\ntest_mse = {}\\ntest_mae = {}\".format(\n",
        "        final_loss_pred, final_mse_pred, final_mae_pred\n",
        "    )\n",
        "    final_saving_txt_path = os.path.join(saving_dir_name, \"{}_evaluation.txt\".format(save_name))\n",
        "    with open(final_saving_txt_path, 'w') as f:\n",
        "        f.write(final_txt_eval)\n",
        "\n",
        "    #make plot for sensor\n",
        "    first_sensors, second_sensors = [], []\n",
        "    for result in predicted_value:\n",
        "        first_sensors.append(result[0])\n",
        "        second_sensors.append(result[1])\n",
        "\n",
        "    # Plot sensor pr2\n",
        "    make_plot(\n",
        "        column_name='pr2(m)',\n",
        "        datetime_main=evaluation_list_data_index,\n",
        "        datetime_pred=evaluation_list_data_index,\n",
        "        value_main=selected_row_data['pr2(m)'].values.tolist(),\n",
        "        value_pred=first_sensors,\n",
        "        save_path=os.path.join(saving_dir_name, '{}_pr2.jpg'.format(save_name))\n",
        "    )\n",
        "\n",
        "    # Plot sensor prs\n",
        "    make_plot(\n",
        "        column_name='prs(m)',\n",
        "        datetime_main=evaluation_list_data_index,\n",
        "        datetime_pred=evaluation_list_data_index,\n",
        "        value_main=selected_row_data['prs(m)'].values.tolist(),\n",
        "        value_pred=second_sensors,\n",
        "        save_path=os.path.join(saving_dir_name, '{}_prs.jpg'.format(save_name))\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUOmcAgMAhNS"
      },
      "outputs": [],
      "source": [
        "separate_eval(month=4, year=2024, save_name=\"evaluasi_5_bulan_pertama\", evaluate_5_months=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MeRXtU3x_f7"
      },
      "outputs": [],
      "source": [
        "separate_eval(month=1, year=2023, save_name=\"evaluasi_5_bulan_pertama\", evaluate_5_months=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqkw_nr0C95v"
      },
      "outputs": [],
      "source": [
        "separate_eval(month=6, year=2023, save_name=\"evaluasi_5_bulan_kedua\", evaluate_5_months=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agjRv3zhDQ85"
      },
      "outputs": [],
      "source": [
        "separate_eval(month=11, year=2023, save_name=\"evaluasi_5_bulan_ketiga\", evaluate_5_months=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFCmM1NhDht2"
      },
      "outputs": [],
      "source": [
        "separate_eval(month=4, year=2024, save_name=\"evaluasi_5_bulan_keempat\", evaluate_5_months=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1DMGrhMliNq"
      },
      "outputs": [],
      "source": [
        "separate_eval(month=9, year=2023, save_name=\"bulan_21\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU4Z8kNxliNq"
      },
      "outputs": [],
      "source": [
        "# bulan 21-25 9/23 hingga 01/24\n",
        "\n",
        "# Bulan 22\n",
        "separate_eval(month=10, year=2023, save_name=\"bulan_22\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK26QUMXliNq"
      },
      "outputs": [],
      "source": [
        "# Bulan 23\n",
        "separate_eval(month=11, year=2023, save_name=\"bulan_23\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZYSdsVYliNz"
      },
      "outputs": [],
      "source": [
        "# Bulan 24\n",
        "separate_eval(month=12, year=2023, save_name=\"bulan_24\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUWk5_-1liNz"
      },
      "outputs": [],
      "source": [
        "# Bulan 25\n",
        "separate_eval(month=1, year=2024, save_name=\"bulan_25\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74450efd"
      },
      "outputs": [],
      "source": [
        "# Ensure df_final is defined (for example, from a CSV file)\n",
        "try:\n",
        "    # Replace 'your_data_file.csv' with the actual data file name\n",
        "    df_final = pd.read_csv('your_data_file.csv')\n",
        "    print(\"df_final loaded successfully. Columns available:\", df_final.columns)\n",
        "except FileNotFoundError:\n",
        "    print(\"File 'your_data_file.csv' not found. Please upload the file or check the path.\")\n",
        "    # Example fallback definition for demonstration purposes\n",
        "    data = {'pr2(m)': [0.1, 0.2, 0.3], 'prs(m)': [0.4, 0.5, 0.6]}\n",
        "    df_final = pd.DataFrame(data)\n",
        "    print(\"Sample df_final created:\", df_final)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQE24mpwBwq2"
      },
      "outputs": [],
      "source": [
        "# Assuming `predicted_result` contains the predictions and has 2 columns\n",
        "# Modify column names if necessary\n",
        "predictions_df = pd.DataFrame(predicted_result, columns=['Predicted Value 1', 'Predicted Value 2'])\n",
        "\n",
        "# Save to Excel\n",
        "predictions_df.to_excel('predicted_results.xlsx', index=False)\n",
        "print(\"Predicted results have been saved to 'predicted_results.xlsx'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "615d6b47"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Download the saved Excel file\n",
        "files.download('predicted_results.xlsx')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sekigahara",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}